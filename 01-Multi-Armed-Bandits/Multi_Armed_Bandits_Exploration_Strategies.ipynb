{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The parameters of the Q-learning algorithm are also taken as user input.\n",
        "\n",
        "• Exploration rate (ϵ): user input (example values: 0.3, 0.5, 0.7)\n",
        "\n",
        "• Learning rate (α): user input (example: 0.3)\n",
        "\n",
        "• Discount factor (γ): user input (example: 0.99)\n",
        "\n",
        "• Number of training episodes: user input (example: 10,000 or more)"
      ],
      "metadata": {
        "id": "fY7tTzQ7ADv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Implement a Q-learning agent in an n × n grid world with user-defined\n",
        "obstacles.\n",
        "\n",
        "  • Example: 5 × 5 grid with obstacles at (1, 1),(1, 3),(3, 1),(3, 3)\n",
        "\n",
        "2. Train the agent for 10,000 episodes (or as per user input) for each of the given values of ϵ (example: 0.3, 0.5, 0.7).\n",
        "\n",
        "3. Track and compute the cumulative reward obtained in each case of ϵ during\n",
        "training."
      ],
      "metadata": {
        "id": "nuIqxaXtAUZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import builtins"
      ],
      "metadata": {
        "id": "vZjgAx6P045j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking input validation\n",
        "def Validation(start_s,goal_s,obstacle,grid) :\n",
        "\n",
        "  if start_s[0] < 0 or start_s[0]>= grid or start_s[1] <0 or start_s[1] >= grid :\n",
        "   print(\"Wrong input\")\n",
        "   exit()\n",
        "\n",
        "  if goal_s[0] < 0 or goal_s[0] >= grid or goal_s[1] < 0 or goal_s[1] >= grid :\n",
        "   print(\"Wrong input\")\n",
        "   exit()\n",
        "\n",
        "  if start_s == goal_s :\n",
        "   print(\"Wrong input \")\n",
        "   exit()\n",
        "\n",
        "  for obs in obstacle :\n",
        "    if obs == start_s or obs == goal_s :\n",
        "     print(\"Wrong input \")\n",
        "     exit()\n",
        "\n",
        "  return True"
      ],
      "metadata": {
        "id": "0aibh--CaFLJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(n, start, goal, obs, alpha, gamma, epsilon, eps, goal_reward, obs_pen, step_pen):\n",
        "    # creating Q-table\n",
        "    q_table = np.zeros((n, n, 4))\n",
        "    cumulative_reward = 0\n",
        "    obs_set = set(obs)\n",
        "    for _ in range(eps):\n",
        "        state = start\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "        while state != goal:\n",
        "            if random.random() < epsilon:\n",
        "                action = random.randint(0, 3)\n",
        "            else:\n",
        "                action = np.argmax(q_table[state[0], state[1]])\n",
        "\n",
        "            dr, dc = actions[action]\n",
        "            next_r = max(0, min(n - 1, state[0] + dr))\n",
        "            next_c = max(0, min(n - 1, state[1] + dc))\n",
        "            next_state = (next_r, next_c)\n",
        "\n",
        "            if next_state in obs_set:\n",
        "                reward = obs_pen\n",
        "                next_state = state\n",
        "            elif next_state == state:\n",
        "                reward = obs_pen\n",
        "            elif next_state == goal:\n",
        "                reward = goal_reward\n",
        "            else:\n",
        "                reward = step_pen\n",
        "\n",
        "            episode_reward += reward\n",
        "\n",
        "            best_next_q = np.max(q_table[next_state[0], next_state[1]])\n",
        "            q_table[state[0], state[1], action] += alpha * (\n",
        "                reward + gamma * best_next_q - q_table[state[0], state[1], action]\n",
        "            )\n",
        "\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "        cumulative_reward += episode_reward\n",
        "\n",
        "    return q_table, cumulative_reward"
      ],
      "metadata": {
        "id": "xsQLtvKALwoi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def action_grid(q_table, n, goal, obs):\n",
        "    grid = [[' ' for i in range(n)] for j in range(n)]\n",
        "    obs_set = set(obs)\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            pos = (i, j)\n",
        "            if pos == goal:\n",
        "                grid[i][j] = 'G'\n",
        "            elif pos in obs_set:\n",
        "                grid[i][j] = 'X'\n",
        "            else:\n",
        "                best_action = np.argmax(q_table[i, j])\n",
        "                grid[i][j] = action_symbols[best_action]\n",
        "    return grid"
      ],
      "metadata": {
        "id": "GK0FQrHByVNL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_q_table(q_table, n):\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            print(f\"State ({i},{j}): {q_table[i, j]}\")"
      ],
      "metadata": {
        "id": "7ekoOLMzyOfA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Action mappings\n",
        "actions = {0: (-1, 0), 1: (0, 1), 2: (1, 0), 3: (0, -1)}  # Up, Right, Down, Left\n",
        "action_symbols = {0: '↑', 1: '→', 2: '↓', 3: '←'}"
      ],
      "metadata": {
        "id": "qfimmS1M3d3B"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Action mappings\n",
        "n_input = builtins.input(\"Enter grid size n : \").strip()\n",
        "n = int(n_input) if n_input else 5\n",
        "\n",
        "start_input = builtins.input(\"Enter start state (row col): \").strip()\n",
        "start = tuple(map(int, start_input.split())) if start_input else (0, 0)\n",
        "\n",
        "goal_input = builtins.input(\"Enter goal state (row col): \").strip()\n",
        "goal = tuple(map(int, goal_input.split())) if goal_input else (4, 4)\n",
        "\n",
        "obstacles_input = builtins.input(\"Enter obstacles (row1 col1,row2 col2,... or  blank): \").strip()\n",
        "obs = [tuple(map(int, o.strip().split())) for o in obstacles_input.split(',') if o.strip()] if obstacles_input else [(1, 1), (1, 3), (3, 1), (3, 3)]\n",
        "\n",
        "goal_reward_input = builtins.input(\"Reward for reaching the goal : \").strip()\n",
        "goal_reward = float(goal_reward_input) if goal_reward_input else 100.0\n",
        "\n",
        "obs_pen_input = builtins.input(\"Penalty for hitting an obstacle/wall : \").strip()\n",
        "obs_pen = float(obs_pen_input) if obs_pen_input else -100.0\n",
        "\n",
        "step_pen_input = builtins.input(\"Penalty for every normal step : \").strip()\n",
        "step_pen = float(step_pen_input) if step_pen_input else -1.0\n",
        "\n",
        "alpha_input = builtins.input(\"Learning rate : \").strip()\n",
        "alpha = float(alpha_input) if alpha_input else 0.3\n",
        "\n",
        "gamma_input = builtins.input(\"Discount factor : \").strip()\n",
        "gamma = float(gamma_input) if gamma_input else 0.99\n",
        "\n",
        "eps_input = builtins.input(\"Num of training episodes : \").strip()\n",
        "eps = int(eps_input) if eps_input else 10000\n",
        "\n",
        "epsilons_input = builtins.input(\"Enter exploration rates separated by commas : \").strip()\n",
        "epsilons = [float(e.strip()) for e in epsilons_input.split(',') if e.strip()] if epsilons_input else [0.3, 0.5, 0.7]\n",
        "\n",
        "if not Validation(start, goal, obs, n): # Corrected arguments passed to Validation\n",
        " print(\"Wrong input\")\n",
        "eps_l = []\n",
        "cumulative_rew = []\n",
        "results = {}\n",
        "for ep in epsilons:\n",
        "    eps_l.append(ep)\n",
        "    q_table, cumulative_reward = training(n, start, goal, obs, alpha, gamma, ep, eps, goal_reward, obs_pen, step_pen)\n",
        "    cumulative_rew.append(cumulative_reward)\n",
        "    best_grid = action_grid(q_table, n, goal, obs)\n",
        "    results[ep] = {\n",
        "            \"q_table\": q_table,\n",
        "            \"best_action_grid\": best_grid,\n",
        "            \"cumulative_reward\": cumulative_reward\n",
        "        }\n",
        "    # Output\n",
        "for ep, data in results.items():\n",
        "        print(f\"\\n Results for epsilon = {ep} \")\n",
        "        print(\"Final Q-Value Table:\")\n",
        "        print_q_table(data[\"q_table\"], n)\n",
        "        print(\"\\nBest Action Grid:\")\n",
        "        for row in data[\"best_action_grid\"]:\n",
        "            print(' '.join(row))\n",
        "        print(f\"\\nTotal Cumulative Reward: {data['cumulative_reward']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MklnoKKdV0h",
        "outputId": "67555c2b-ee91-4ff8-fd29-13b33b7fa154"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter grid size n : 4 \n",
            "Enter start state (row col): 0 0\n",
            "Enter goal state (row col): 3 3\n",
            "Enter obstacles (row1 col1,row2 col2,... or  blank): 1 1,2 3\n",
            "Reward for reaching the goal : 150\n",
            "Penalty for hitting an obstacle/wall : -20\n",
            "Penalty for every normal step : -1\n",
            "Learning rate : 0.4\n",
            "Discount factor : 0.99\n",
            "Num of training episodes : 10000\n",
            "Enter exploration rates separated by commas : 0.3,0.4,0.5,0.6,0.7\n",
            "\n",
            " Results for epsilon = 0.3 \n",
            "Final Q-Value Table:\n",
            "State (0,0): [116.37003735 137.74751247 137.74751247 116.37003735]\n",
            "State (0,1): [118.74751247 140.1490025  118.74751247 135.37003735]\n",
            "State (0,2): [121.1490025  137.74751247 142.57475    137.74751247]\n",
            "State (0,3): [118.74751247 118.74751247 140.1490025  140.1490025 ]\n",
            "State (1,0): [135.37003735 118.74751247 140.1490025  118.74751247]\n",
            "State (1,1): [0. 0. 0. 0.]\n",
            "State (1,2): [140.1490025 140.1490025 145.025     123.57475  ]\n",
            "State (1,3): [137.74751247 121.1490025  121.1490025  142.57475   ]\n",
            "State (2,0): [137.74751247 142.57475    142.57475    121.1490025 ]\n",
            "State (2,1): [123.57475   145.025     145.025     140.1490025]\n",
            "State (2,2): [142.57475 126.025   147.5     142.57475]\n",
            "State (2,3): [0. 0. 0. 0.]\n",
            "State (3,0): [140.1490025 145.025     123.57475   123.57475  ]\n",
            "State (3,1): [142.57475 147.5     126.025   142.57475]\n",
            "State (3,2): [145.025 150.    128.5   145.025]\n",
            "State (3,3): [0. 0. 0. 0.]\n",
            "\n",
            "Best Action Grid:\n",
            "→ → ↓ ↓\n",
            "↓ X ↓ ←\n",
            "→ → ↓ X\n",
            "→ → → G\n",
            "\n",
            "Total Cumulative Reward: 1252120.0\n",
            "\n",
            " Results for epsilon = 0.4 \n",
            "Final Q-Value Table:\n",
            "State (0,0): [116.37003735 137.74751247 137.74751247 116.37003735]\n",
            "State (0,1): [118.74751247 140.1490025  118.74751247 135.37003735]\n",
            "State (0,2): [121.1490025  137.74751247 142.57475    137.74751247]\n",
            "State (0,3): [118.74751247 118.74751247 140.1490025  140.1490025 ]\n",
            "State (1,0): [135.37003735 118.74751247 140.1490025  118.74751247]\n",
            "State (1,1): [0. 0. 0. 0.]\n",
            "State (1,2): [140.1490025 140.1490025 145.025     123.57475  ]\n",
            "State (1,3): [137.74751247 121.1490025  121.1490025  142.57475   ]\n",
            "State (2,0): [137.74751247 142.57475    142.57475    121.1490025 ]\n",
            "State (2,1): [123.57475   145.025     145.025     140.1490025]\n",
            "State (2,2): [142.57475 126.025   147.5     142.57475]\n",
            "State (2,3): [0. 0. 0. 0.]\n",
            "State (3,0): [140.1490025 145.025     123.57475   123.57475  ]\n",
            "State (3,1): [142.57475 147.5     126.025   142.57475]\n",
            "State (3,2): [145.025 150.    128.5   145.025]\n",
            "State (3,3): [0. 0. 0. 0.]\n",
            "\n",
            "Best Action Grid:\n",
            "→ → ↓ ↓\n",
            "↓ X ↓ ←\n",
            "→ → ↓ X\n",
            "→ → → G\n",
            "\n",
            "Total Cumulative Reward: 1135920.0\n",
            "\n",
            " Results for epsilon = 0.5 \n",
            "Final Q-Value Table:\n",
            "State (0,0): [116.37003735 137.74751247 137.74751247 116.37003735]\n",
            "State (0,1): [118.74751247 140.1490025  118.74751247 135.37003735]\n",
            "State (0,2): [121.1490025  137.74751247 142.57475    137.74751247]\n",
            "State (0,3): [118.74751247 118.74751247 140.1490025  140.1490025 ]\n",
            "State (1,0): [135.37003735 118.74751247 140.1490025  118.74751247]\n",
            "State (1,1): [0. 0. 0. 0.]\n",
            "State (1,2): [140.1490025 140.1490025 145.025     123.57475  ]\n",
            "State (1,3): [137.74751247 121.1490025  121.1490025  142.57475   ]\n",
            "State (2,0): [137.74751247 142.57475    142.57475    121.1490025 ]\n",
            "State (2,1): [123.57475   145.025     145.025     140.1490025]\n",
            "State (2,2): [142.57475 126.025   147.5     142.57475]\n",
            "State (2,3): [0. 0. 0. 0.]\n",
            "State (3,0): [140.1490025 145.025     123.57475   123.57475  ]\n",
            "State (3,1): [142.57475 147.5     126.025   142.57475]\n",
            "State (3,2): [145.025 150.    128.5   145.025]\n",
            "State (3,3): [0. 0. 0. 0.]\n",
            "\n",
            "Best Action Grid:\n",
            "→ → ↓ ↓\n",
            "↓ X ↓ ←\n",
            "→ → ↓ X\n",
            "→ → → G\n",
            "\n",
            "Total Cumulative Reward: 972630.0\n",
            "\n",
            " Results for epsilon = 0.6 \n",
            "Final Q-Value Table:\n",
            "State (0,0): [116.37003735 137.74751247 137.74751247 116.37003735]\n",
            "State (0,1): [118.74751247 140.1490025  118.74751247 135.37003735]\n",
            "State (0,2): [121.1490025  137.74751247 142.57475    137.74751247]\n",
            "State (0,3): [118.74751247 118.74751247 140.1490025  140.1490025 ]\n",
            "State (1,0): [135.37003735 118.74751247 140.1490025  118.74751247]\n",
            "State (1,1): [0. 0. 0. 0.]\n",
            "State (1,2): [140.1490025 140.1490025 145.025     123.57475  ]\n",
            "State (1,3): [137.74751247 121.1490025  121.1490025  142.57475   ]\n",
            "State (2,0): [137.74751247 142.57475    142.57475    121.1490025 ]\n",
            "State (2,1): [123.57475   145.025     145.025     140.1490025]\n",
            "State (2,2): [142.57475 126.025   147.5     142.57475]\n",
            "State (2,3): [0. 0. 0. 0.]\n",
            "State (3,0): [140.1490025 145.025     123.57475   123.57475  ]\n",
            "State (3,1): [142.57475 147.5     126.025   142.57475]\n",
            "State (3,2): [145.025 150.    128.5   145.025]\n",
            "State (3,3): [0. 0. 0. 0.]\n",
            "\n",
            "Best Action Grid:\n",
            "→ → ↓ ↓\n",
            "↓ X ↓ ←\n",
            "→ → ↓ X\n",
            "→ → → G\n",
            "\n",
            "Total Cumulative Reward: 738414.0\n",
            "\n",
            " Results for epsilon = 0.7 \n",
            "Final Q-Value Table:\n",
            "State (0,0): [116.37003735 137.74751247 137.74751247 116.37003735]\n",
            "State (0,1): [118.74751247 140.1490025  118.74751247 135.37003735]\n",
            "State (0,2): [121.1490025  137.74751247 142.57475    137.74751247]\n",
            "State (0,3): [118.74751247 118.74751247 140.1490025  140.1490025 ]\n",
            "State (1,0): [135.37003735 118.74751247 140.1490025  118.74751247]\n",
            "State (1,1): [0. 0. 0. 0.]\n",
            "State (1,2): [140.1490025 140.1490025 145.025     123.57475  ]\n",
            "State (1,3): [137.74751247 121.1490025  121.1490025  142.57475   ]\n",
            "State (2,0): [137.74751247 142.57475    142.57475    121.1490025 ]\n",
            "State (2,1): [123.57475   145.025     145.025     140.1490025]\n",
            "State (2,2): [142.57475 126.025   147.5     142.57475]\n",
            "State (2,3): [0. 0. 0. 0.]\n",
            "State (3,0): [140.1490025 145.025     123.57475   123.57475  ]\n",
            "State (3,1): [142.57475 147.5     126.025   142.57475]\n",
            "State (3,2): [145.025 150.    128.5   145.025]\n",
            "State (3,3): [0. 0. 0. 0.]\n",
            "\n",
            "Best Action Grid:\n",
            "→ → ↓ ↓\n",
            "↓ X ↓ ←\n",
            "→ → ↓ X\n",
            "→ → → G\n",
            "\n",
            "Total Cumulative Reward: 321872.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0e46cf5c",
        "outputId": "b3174a72-48ce-431f-d46b-4f7be348c5b8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {'Epsilon': list(results.keys()), 'Cumulative Reward': [data['cumulative_reward'] for data in results.values()]}\n",
        "\n",
        "df_results = pd.DataFrame(data)\n",
        "\n",
        "display(df_results)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Epsilon  Cumulative Reward\n",
              "0      0.3          1252120.0\n",
              "1      0.4          1135920.0\n",
              "2      0.5           972630.0\n",
              "3      0.6           738414.0\n",
              "4      0.7           321872.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a9fb1650-e2eb-4ba4-bb13-ddac1aee2947\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epsilon</th>\n",
              "      <th>Cumulative Reward</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.3</td>\n",
              "      <td>1252120.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.4</td>\n",
              "      <td>1135920.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.5</td>\n",
              "      <td>972630.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.6</td>\n",
              "      <td>738414.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.7</td>\n",
              "      <td>321872.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9fb1650-e2eb-4ba4-bb13-ddac1aee2947')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a9fb1650-e2eb-4ba4-bb13-ddac1aee2947 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a9fb1650-e2eb-4ba4-bb13-ddac1aee2947');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-fc27174f-6a3c-4b21-ad0d-c9f74db9db89\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc27174f-6a3c-4b21-ad0d-c9f74db9db89')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-fc27174f-6a3c-4b21-ad0d-c9f74db9db89 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_eb754273-e4f0-430b-a917-feac3cedb561\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_eb754273-e4f0-430b-a917-feac3cedb561 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Epsilon\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15811388300841894,\n        \"min\": 0.3,\n        \"max\": 0.7,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.4,\n          0.7,\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cumulative Reward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 368786.6207893123,\n        \"min\": 321872.0,\n        \"max\": 1252120.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1135920.0,\n          321872.0,\n          972630.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# • **How does the exploration rate (ϵ) affect learning?**\n",
        "\n",
        "#• **Does higher exploration find better paths or slow down convergence?**"
      ],
      "metadata": {
        "id": "L3N_K1lWdjzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 1: The exploration rate (ε) decides how much the agent explores new moves versus sticking to known good ones. From your results, a lower ε (0.30) boosts the cumulative reward by letting the agent rely more on learned paths, preventing it from getting hit by obstacles and hitting the goal fast. Higher ε values(0.7) , mean more random moves, leading to more obstacle bumps and lower rewards due to penalties.\n",
        "\n",
        "\n",
        "\n",
        "Ans 2: As Higher exploration (ε = 0.7) means more exploration, which makes one more prone to hitting the obstacles, resulting in a low cumulative reward, which shows it slows convergence. Lower ε (0.3) suggests quicker convergence on a solid path, though it might miss the best route. Middle ground (ε = 0.5) balances both but still lags behind the lowest ε, hinting that for this nxn grid, less exploration works better."
      ],
      "metadata": {
        "id": "ZDwfiWlfdrKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Write a short explanation (2–3 paragraphs) discussing how the exploration rate influenced the learning outcomes.**"
      ],
      "metadata": {
        "id": "cFrFdz_Kd5Bk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: The exploration rate (ε) sets the tone for how the agent figures things out. With ε = 0.3 scoring the top reward, the agent tends to stick with a greedy path, preventing obstacles. This fast lock-on to a solid path shows low exploration, which works perfectly here, though it might miss a shorter route if it doesn’t explore around enough at the start.\n",
        "\n",
        "On the other hand, jacking ε up to 0.7 crashes the reward, those random moves sum up penalties and slow everything down big time, it could uncover better paths in a trickier setup, but it just messes with the flow.\n",
        "\n",
        "The middle ground at ε = 0.5 tries to play it in between but doesn’t get the path which is shortest and optimal. For this nxn grid, keeping ε low at 0.3 seems to be the correct move for quick learning and optimal rewards."
      ],
      "metadata": {
        "id": "IJQzPsUTd5jD"
      }
    }
  ]
}