# Multi-Armed Bandits: Exploration-Exploitation Trade-off

## Overview
A study on regret minimization in stochastic environments using classical bandit algorithms.

## Mathematical & Technical Foundations
- Regret Analysis: Theoretical comparison of cumulative regret bounds.
- Upper Confidence Bound (UCB): Implementing confidence intervals using the Hoeffding Inequality.
- Epsilon-Greedy: Balancing exploration vs. exploitation through a probability parameter.

## Implementation Details
- Comparative study of Epsilon-Greedy, UCB, and Thompson Sampling.
- Simulation across stationary Gaussian reward distributions.
- Analysis of convergence rates based on Cumulative Regret curves.

## Technologies
- PyTorch, Gymnasium (OpenAI Gym), NumPy, Matplotlib
