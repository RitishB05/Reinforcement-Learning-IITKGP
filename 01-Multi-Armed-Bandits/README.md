# Multi-Armed Bandits: Exploration-Exploitation Strategies

## Project Overview
An in-depth study of the fundamental exploration-exploitation trade-off using classical Bandit algorithms.

## Mathematical Foundations
- Regret Bounds: Comparison of cumulative regret across epsilon-greedy, UCB, and Thompson Sampling.
- UCB Algorithm: Implementing Upper Confidence Bounds using Hoeffding's Inequality to minimize regret.
- Thompson Sampling: Bayesian approach using Beta distributions for optimal action selection.

## Implementation Details
Evaluation focused on stationary vs. non-stationary reward distributions, demonstrating how the agent adapts its internal value estimates over 10,000+ iterations.

## Technologies Used
- Python, PyTorch, Gymnasium (OpenAI Gym), NumPy, Matplotlib
